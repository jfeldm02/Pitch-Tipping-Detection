{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitch Tipping Via Human Pose Estimation and Time Series Classification by Justin Feldman \n",
    "## Classification Modeling and Analysis:\n",
    "#### This file is designed to help you evaluate whether there is actionable pitch tipping insights or not. The various test % results indicate if the models trained on your pitcher's biomechanic data can accurately predict what pitch the pitcher is throwing before the point of release.\n",
    "\n",
    "#### 1. Multinomial Logistic Regression\n",
    "#### 2. Support Vector Machines\n",
    "#### 3. Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input your data in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitcher_name = 'First Last' \n",
    "processed_data_path = '.../your_path_processed.json'\n",
    "save_your_models = '.../your_path' # Directory to save models to for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def load_processed_data(json_file_path):\n",
    "    \"\"\"\n",
    "    Load processed data from a JSON file back into a list of dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    json_file_path (str): Path to the JSON file containing processed data\n",
    "    \n",
    "    Returns:\n",
    "    list: List of dictionaries, each containing a dataframe and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Convert back to the original format\n",
    "    dataframes_processed = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Convert the dict records back to a DataFrame\n",
    "        df = pd.DataFrame(item['dataframe'])\n",
    "        \n",
    "        # Create the same structure as the original dataframes_processed\n",
    "        dataframes_processed.append({\n",
    "            'dataframe': df,\n",
    "            'pitch_type': item['pitch_type'],\n",
    "            'video_index': item['video_index'],\n",
    "            'filename': item['filename']\n",
    "        })\n",
    "    \n",
    "    return dataframes_processed\n",
    "\n",
    "def flatten_data_for_analysis(dataframes):\n",
    "    \n",
    "    # Lists to store features and targets\n",
    "    all_features = []\n",
    "    all_targets = []\n",
    "    landmarks = [\n",
    "        'left_shoulder',\n",
    "        'right_shoulder',\n",
    "        'left_elbow',\n",
    "        'right_elbow',\n",
    "        'left_hip',\n",
    "        'right_hip',\n",
    "        'left_knee',\n",
    "        'right_knee',\n",
    "        'face',\n",
    "        'left_hand',\n",
    "        'right_hand',\n",
    "        'left_foot',\n",
    "        'right_foot'\n",
    "        ]\n",
    "\n",
    "    # Process each video\n",
    "    for video_data in dataframes:\n",
    "        pitch_type = video_data['pitch_type']\n",
    "        df = video_data['dataframe']\n",
    "        \n",
    "        # Initialize a list for this video's flattened features\n",
    "        video_features = []\n",
    "        \n",
    "        # For each frame, extract and flatten selected landmark coordinates\n",
    "        for frame_idx in range(len(df)):\n",
    "            frame_data = df.iloc[frame_idx]\n",
    "            \n",
    "            # Extract coordinates for selected landmarks\n",
    "            for landmark in landmarks:\n",
    "                # Find the relevant coordinate columns for this landmark\n",
    "                x_col = f'{landmark}_x'\n",
    "                y_col = f'{landmark}_y'\n",
    "                presence_col = f'{landmark}_presence'\n",
    "                \n",
    "                # Add x and y coordinates to features\n",
    "                video_features.append(frame_data[x_col])\n",
    "                video_features.append(frame_data[y_col])\n",
    "                video_features.append(frame_data[presence_col])\n",
    "        \n",
    "        # Add this video's features to the lists\n",
    "        all_features.append(video_features)\n",
    "        all_targets.append(pitch_type)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    X_pitches = pd.DataFrame(all_features)\n",
    "    y_pitches = pd.Series(all_targets, name='pitch_type')\n",
    "    \n",
    "    return X_pitches, y_pitches\n",
    "\n",
    "def save_model(model, model_name, save_flag=False, directory='saved_models'):\n",
    "    \"\"\"\n",
    "    Saves your model for future use. \n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model to be saved\n",
    "    model_name (str): Name of the model as it will be saved\n",
    "    save_flag (bool): True to save, False to ignore\n",
    "    directory (str): Desired file path to save model to \n",
    "    \n",
    "    \"\"\"\n",
    "    if not save_flag:\n",
    "        return None\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "    \n",
    "    # Generate timestamp for unique filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create filename with model name and timestamp\n",
    "    filename = f\"{model_name}_{timestamp}.pkl\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(model, filepath)\n",
    "    print(f\"Model '{model_name}' saved to {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "dataframes = load_processed_data(processed_data_path)\n",
    "X_pitches, y_pitches = flatten_data_for_analysis(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This section implements a standard multiclassification method:\n",
    "\n",
    "- **Multinomial Logistic Regression**: Effective for classifying linearly separable and distinct high dimensional data. [Read more...](https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20supervised,based%20on%20patient%20test%20results.)\n",
    "\n",
    "Note: Given the temporal nature, overall complexity, and nonlinearity of your pitcher's dataset, the expectation is that you will not see good accuracy results from support vector machine analysis. The likely result is a significantly lower test % than train % indicating an inability to converge to accurate class separation. A high train % value is likely a result of data overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression setup and running. Do not touch this cell \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def log_reg_model_setup(loss, X_pitches, y_pitches):\n",
    "    \"\"\"\n",
    "    Setting up a logistic regression classifier pipeline. \n",
    "    \n",
    "    Parameters:\n",
    "    loss (str): Type of training loss\n",
    "    X_pitches (dataframe): Flattened pitch data\n",
    "    y_pitches (series): Respective X_pitches labels\n",
    "    \n",
    "    Outputs:\n",
    "    reg_model: Trained logistic regression model\n",
    "    reg_model_train_accuracy (float): 0-1 training accuracy\n",
    "    reg_model_test_accuracy (float): 0-1 test set accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_pitches, y_pitches, stratify=y_pitches)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train, x_test = scaler.fit_transform(x_train), scaler.fit_transform(x_test)\n",
    "\n",
    "    if loss == 'l1':\n",
    "        reg_model = LogisticRegression(penalty=loss, solver='liblinear')\n",
    "    elif loss == 'elasticnet':\n",
    "        reg_model = LogisticRegression(penalty=loss, solver='saga', l1_ratio=0.5)\n",
    "    else:  \n",
    "        reg_model = LogisticRegression(penalty=loss, solver='lbfgs')\n",
    "    \n",
    "    reg_model = reg_model.fit(x_train,y_train)\n",
    "\n",
    "    reg_model_train_accuracy = reg_model.score(x_train, y_train)\n",
    "    reg_model_test_accuracy = reg_model.score(x_test, y_test)\n",
    "\n",
    "    return reg_model, reg_model_train_accuracy, reg_model_test_accuracy\n",
    "\n",
    "def log_reg_plot(l1_model_train_accuracy, l1_model_test_accuracy,l2_model_train_accuracy, l2_model_test_accuracy, elasticnet_model_train_accuracy, elasticnet_model_test_accuracy, pitcher_name):\n",
    "    \"\"\"\n",
    "    Plotting the model results. \n",
    "    \n",
    "    Parameters:\n",
    "    model_train and model_test accuracies (float): Model train and test accuracies across different loss functions. \n",
    "    \n",
    "    \"\"\"\n",
    "    penalty_types = ['L1', 'L2', 'ElasticNet']\n",
    "\n",
    "    train_accuracies = [l1_model_train_accuracy, l2_model_train_accuracy, elasticnet_model_train_accuracy]\n",
    "    test_accuracies = [l1_model_test_accuracy, l2_model_test_accuracy, elasticnet_model_test_accuracy]\n",
    "\n",
    "    train_accuracies = [acc * 100 for acc in train_accuracies]\n",
    "    test_accuracies = [acc * 100 for acc in test_accuracies]\n",
    "\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(penalty_types))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    train_bars = ax.bar(x - bar_width/2, train_accuracies, bar_width, label='Train Accuracy', color='blue')\n",
    "    test_bars = ax.bar(x + bar_width/2, test_accuracies, bar_width, label='Test Accuracy', color='green')\n",
    "\n",
    "    ax.set_xlabel('Penalty Type')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'Logistic Regression Performance: Train vs Test Accuracy\\n{pitcher_name}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(penalty_types)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_ylim(0, max(max(train_accuracies), max(test_accuracies)) * 1.1)\n",
    "\n",
    "    def add_labels(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.1f}%',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3), \n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "    add_labels(train_bars)\n",
    "    add_labels(test_bars)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def log_reg_report(log_reg_model_train_accuracy, log_reg_model_test_accuracy, penalty):\n",
    "    \"\"\"\n",
    "    An initial reporting function. \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"\"\"\n",
    "    +{'-'*20}+{'-'*12}+{'-'*12}+\n",
    "    | {'Loss: ' + penalty:<18} | {'Train (%)':^10} | {'Test (%)':^10} |\n",
    "    +{'-'*20}+{'-'*12}+{'-'*12}+\n",
    "    | {'Accuracy':^18} | {int(np.mean(log_reg_model_train_accuracy)*100):^10} | {int(np.mean(log_reg_model_test_accuracy)*100):^10} |\n",
    "    +{'-'*20}+{'-'*12}+{'-'*12}+\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "l1_model, l1_model_train_accuracy, l1_model_test_accuracy = log_reg_model_setup(loss= 'l1', X_pitches=X_pitches, y_pitches=y_pitches)\n",
    "l2_model, l2_model_train_accuracy, l2_model_test_accuracy = log_reg_model_setup(loss= 'l2', X_pitches=X_pitches, y_pitches=y_pitches)\n",
    "elasticnet_model, elasticnet_model_train_accuracy, elasticnet_model_test_accuracy = log_reg_model_setup(loss= 'elasticnet', X_pitches=X_pitches, y_pitches=y_pitches)\n",
    "\n",
    "# log_reg_report(l1_model_train_accuracy, l1_model_test_accuracy, 'l1')\n",
    "# log_reg_report(l2_model_train_accuracy, l2_model_test_accuracy, 'l2')\n",
    "# log_reg_report(elasticnet_model_train_accuracy, elasticnet_model_test_accuracy, 'elasticnet')\n",
    "log_reg_plot(l1_model_train_accuracy, l1_model_test_accuracy,l2_model_train_accuracy, l2_model_test_accuracy, elasticnet_model_train_accuracy, elasticnet_model_test_accuracy,pitcher_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set flag to 'True' if you want to save these models for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = False\n",
    "\n",
    "save_model(l1_model, 'log_reg_l1', save_flag, directory=save_your_models)\n",
    "save_model(l2_model, 'log_reg_l2', save_flag, directory=save_your_models)\n",
    "save_model(elasticnet_model, 'log_reg_elastic', save_flag, directory=save_your_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM's\n",
    "\n",
    "This section implements a more flexible classifier than the logistic regression model:\n",
    "\n",
    "- **Support Vector Machines (SVM)**: Effective for capturing classifications in higher dimensions with better handling of outlier data than logistic regression. [Read More...](https://www.ibm.com/think/topics/support-vector-machine)\n",
    "\n",
    "Note: Given the temporal nature, overall complexity, and nonlinearity of your pitcher's dataset, the expectation is that you will not see good accuracy results from support vector machine analysis. The likely result is a significantly lower test % than train % indicating an inability to converge to accurate class separation. A high train % value is likely the result of data overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM setup and running. Do not touch this cell \n",
    "def svm_model_setup(kernel, X_pitches, y_pitches):\n",
    "    \"\"\"\n",
    "    Setting up a SVM classifier pipeline. \n",
    "    \n",
    "    Parameters:\n",
    "    kernel (str): Type of kernel\n",
    "    X_pitches (dataframe): Flattened pitch data\n",
    "    y_pitches (series): Respective X_pitches labels\n",
    "    \n",
    "    Outputs:\n",
    "    svm_model: Trained SVM model\n",
    "    svm_model_train_accuracy (float): 0-1 training accuracy\n",
    "    svm_model_test_accuracy (float): 0-1 test set accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_pitches, y_pitches,stratify=y_pitches)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train, x_test = scaler.fit_transform(x_train), scaler.fit_transform(x_test)\n",
    "    \n",
    "    svm_model = svm.SVC(kernel=kernel)\n",
    "    svm_model = svm_model.fit(x_train,y_train)\n",
    "    \n",
    "    svm_model_train_accuracy = svm_model.score(x_train, y_train)\n",
    "    svm_model_test_accuracy = svm_model.score(x_test, y_test)\n",
    "\n",
    "    return svm_model, svm_model_train_accuracy, svm_model_test_accuracy\n",
    "\n",
    "def svm_plot(linear_model_train_accuracy, linear_model_test_accuracy,poly_model_train_accuracy, poly_model_test_accuracy,rbf_model_train_accuracy, rbf_model_test_accuracy, pitcher_name):\n",
    "    \"\"\"\n",
    "    Plotting the model results. \n",
    "    \n",
    "    Parameters:\n",
    "    model_train and model_test accuracies (float): Model train and test accuracies across different loss functions. \n",
    "    \n",
    "    \"\"\"\n",
    "    kernels = ['Linear', 'Poly', 'RBF']\n",
    "\n",
    "    train_accuracies = [linear_model_train_accuracy, poly_model_train_accuracy, rbf_model_train_accuracy]\n",
    "    test_accuracies = [linear_model_test_accuracy, poly_model_test_accuracy, rbf_model_test_accuracy]\n",
    "\n",
    "    train_accuracies = [acc * 100 for acc in train_accuracies]\n",
    "    test_accuracies = [acc * 100 for acc in test_accuracies]\n",
    "\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(kernels))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    train_bars = ax.bar(x - bar_width/2, train_accuracies, bar_width, label='Train Accuracy', color='blue')\n",
    "    test_bars = ax.bar(x + bar_width/2, test_accuracies, bar_width, label='Test Accuracy', color='green')\n",
    "\n",
    "    ax.set_xlabel('Kernel Type')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'SVM Model Performance: Train vs Test Accuracy\\n{pitcher_name}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(kernels)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_ylim(0, max(max(train_accuracies), max(test_accuracies)) * 1.1)\n",
    "\n",
    "    def add_labels(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.1f}%',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "    add_labels(train_bars)\n",
    "    add_labels(test_bars)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def svm_report(svm_model_train_accuracy, svm_model_test_accuracy, kernel):\n",
    "    print(f\"\"\"\n",
    "    +{'-'*20}+{'-'*12}+{'-'*12}+\n",
    "    | {'Kernel: ' + kernel:<18} | {'Train (%)':^10} | {'Test (%)':^10} |\n",
    "    +{'-'*20}+{'-'*12}+{'-'*12}+\n",
    "    | {'Accuracy':^18} | {int(np.mean(svm_model_train_accuracy)*100):^10} | {int(np.mean(svm_model_test_accuracy)*100):^10} |\n",
    "    +{'-'*20}+{'-'*12}+{'-'*12}+\n",
    "    \"\"\")\n",
    "\n",
    "svm_model_linear, linear_model_train_accuracy, linear_model_test_accuracy = svm_model_setup(kernel='linear', X_pitches=X_pitches, y_pitches=y_pitches)\n",
    "svm_model_poly, poly_model_train_accuracy, poly_model_test_accuracy = svm_model_setup(kernel= 'poly', X_pitches=X_pitches, y_pitches=y_pitches)\n",
    "svm_model_rbf, rbf_model_train_accuracy, rbf_model_test_accuracy = svm_model_setup(kernel= 'rbf', X_pitches=X_pitches, y_pitches=y_pitches)\n",
    "\n",
    "# svm_report(linear_model_train_accuracy, linear_model_test_accuracy, 'Linear')\n",
    "# svm_report(poly_model_train_accuracy, poly_model_test_accuracy, 'Poly')\n",
    "# svm_report(rbf_model_train_accuracy, rbf_model_test_accuracy, 'RBF')\n",
    "\n",
    "svm_plot(linear_model_train_accuracy, linear_model_test_accuracy,poly_model_train_accuracy, poly_model_test_accuracy,rbf_model_train_accuracy, rbf_model_test_accuracy,pitcher_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set flag to 'True' if you want to save these models for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = False\n",
    "\n",
    "save_model(svm_model_linear, 'svm_linear_kernel', save_flag, directory=save_your_models)\n",
    "save_model(svm_model_poly, 'svm_poly_kernel', save_flag, directory=save_your_models)\n",
    "save_model(svm_model_rbf, 'svm_rbf_kernel', save_flag, directory=save_your_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN and LSTM Neural Networks\n",
    "\n",
    "This section implements two powerful sequential data processing architectures:\n",
    "\n",
    "- **Recurrent Neural Network (RNN)**: Effective for capturing short-term dependencies [Read more...](https://www.ibm.com/think/topics/recurrent-neural-networks)\n",
    "- **Long Short-Term Memory (LSTM)**: Specialized for handling long-term dependencies [Read more...](https://medium.com/@rebeen.jaff/what-is-lstm-introduction-to-long-short-term-memory-66bd3855b9ce)\n",
    "\n",
    "### Genetic Algorithm Hyperparameter Optimization\n",
    "\n",
    "We employ an evolutionary approach to discover optimal model configurations automatically. [Read more...](https://www.justinafeldman.com/music-reconstruction-using-genetic-alg)\n",
    "\n",
    "> *\"Genetic algorithms are a robust genre of algorithms inspired by the theories of natural selection and evolution. Given a population of individuals represented as a structured data set, the genetic algorithm architecture creates an artificial natural selection environment in which the strongest individuals survive based on the strength of their 'genetics'. These surviving individuals remain in the population to produce offspring that carry combinations of the parents' genetics along with novel genetic mutations as they occur naturally in living beings. These offspring potentially possess a stronger genetic composition, improving the population's traits overall as the evolutionary process continues.\"* \n",
    "> \n",
    "> — Jiwani, Feldman et al. (2024)\n",
    "\n",
    "### Genetic Encoding\n",
    "\n",
    "Each model \"gene\" in the population encodes the following hyperparameters:\n",
    "\n",
    "| Parameter | Possible Values |\n",
    "|-----------|----------------|\n",
    "| Learning Rate | 0.0001, 0.0005, 0.001, 0.005, 0.01 |\n",
    "| Hidden Layer Depth | 1, 2, 3, 4, 5 |\n",
    "| Hidden Layer Size | 16, 32, 64, 128, 256 |\n",
    "| Training Epochs | 5-50 |\n",
    "| Architecture Type | RNN or LSTM |\n",
    "\n",
    "### Fitness Function\n",
    "\n",
    "The fitness evaluation prioritizes:\n",
    "1. **Maximizing predictive performance**\n",
    "2. **Avoiding overfitting**\n",
    "\n",
    "The evolution process will iteratively select the best-performing models and combine their characteristics to discover increasingly effective architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        logit = self.fc(out[:, -1, :])\n",
    "        prob = nn.functional.softmax(logit, dim=1)\n",
    "    \n",
    "        return prob, logit\n",
    "    \n",
    "# LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        logit = self.fc(out[:, -1, :])\n",
    "        prob = nn.functional.softmax(logit, dim=1)\n",
    "    \n",
    "        return prob, logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataframes):\n",
    "    \"\"\"\n",
    "    Structures data to enter genetic algorithm pipeline. \n",
    "    \n",
    "    Parameters:\n",
    "    dataframes (dataframe): Data from input file\n",
    "\n",
    "    Outputs:\n",
    "    X (np.arr): Arrays of flattened samples\n",
    "    y (np.arr): Respective classes \n",
    "    pitch_type_to_idx (dict): Creates a set of pitch types mapped to a value\n",
    "    idx_to_pitch_type (dict): Reciprocal \n",
    "    \n",
    "    \"\"\"\n",
    "    # Extract features and labels\n",
    "    X = []  # List to store feature tensors\n",
    "    y = []  # List to store labels\n",
    "    \n",
    "    # Create a mapping for pitch types to numeric indices\n",
    "    unique_pitch_types = set(item['pitch_type'] for item in dataframes)\n",
    "    pitch_type_to_idx = {pitch: idx for idx, pitch in enumerate(sorted(unique_pitch_types))}\n",
    "    idx_to_pitch_type = {idx: pitch for pitch, idx in pitch_type_to_idx.items()}\n",
    "    \n",
    "    # Process each dataframe\n",
    "    for item in dataframes:\n",
    "        # Get the dataframe with landmark coordinates\n",
    "        df = item['dataframe']\n",
    "        \n",
    "        # Convert dataframe to numpy array (frames × features)\n",
    "        features = df.values.astype(np.float32)\n",
    "\n",
    "        # Normalize per sample\n",
    "        mean = np.mean(features, axis=0, keepdims=True)\n",
    "        std = np.std(features, axis=0, keepdims=True) + 1e-8 \n",
    "        features_normalized = (features - mean) / std\n",
    "\n",
    "        # Get the label (pitch type)\n",
    "        label = pitch_type_to_idx[item['pitch_type']]\n",
    "        \n",
    "        X.append(features_normalized)\n",
    "        y.append(label)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X) # (num_samples, num_frames, num_features)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y, pitch_type_to_idx, idx_to_pitch_type\n",
    "\n",
    "# Gene class to represent a set of hyperparameters\n",
    "class Gene:\n",
    "    \"\"\"\n",
    "    Structures a gene and initializes first generation for testing. Refer to readme or report for more info. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, model_type=None, num_epochs=None, lr=None, hidden_size=None, num_layers=None):\n",
    "        # Initialize with random values if not provided\n",
    "        self.model_type = model_type if model_type is not None else random.choice([RNN, LSTM])\n",
    "        self.num_epochs = num_epochs if num_epochs is not None else random.randint(5, 50)\n",
    "        self.lr = lr if lr is not None else random.choice([0.0001, 0.0005, 0.001, 0.005, 0.01])\n",
    "        self.hidden_size = hidden_size if hidden_size is not None else random.choice([16, 32, 64, 128, 256])\n",
    "        self.num_layers = num_layers if num_layers is not None else random.randint(1, 5)\n",
    "        self.fitness = 0\n",
    "        self.train_accuracy = 0\n",
    "        self.test_accuracy = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        model_name = \"RNN\" if self.model_type == RNN else \"LSTM\"\n",
    "        return (f\"Model: {model_name}, Epochs: {self.num_epochs}, LR: {self.lr}, \"\n",
    "                f\"Hidden Size: {self.hidden_size}, Layers: {self.num_layers}, \"\n",
    "                f\"Fitness: {self.fitness:.4f}, Train Acc: {self.train_accuracy:.4f}, Test Acc: {self.test_accuracy:.4f}\")\n",
    "    \n",
    "    def to_dict(self):\n",
    "        model_name = \"RNN\" if self.model_type == RNN else \"LSTM\"\n",
    "        return {\n",
    "            \"model_type\": model_name,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"learning_rate\": self.lr,\n",
    "            \"hidden_size\": self.hidden_size,\n",
    "            \"num_layers\": self.num_layers,\n",
    "            \"fitness\": self.fitness,\n",
    "            \"train_accuracy\": self.train_accuracy,\n",
    "            \"test_accuracy\": self.test_accuracy\n",
    "        }\n",
    "\n",
    "# Genetic Algorithm class\n",
    "class GeneticAlgorithm:\n",
    "    \"\"\"\n",
    "    Structures the genetic algorithm process.  \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, X_data, y_data, input_size, num_classes, \n",
    "                 population_size=10, num_generations=5, \n",
    "                 mutation_rate=0.2, crossover_rate=0.7,\n",
    "                 batch_size=16, test_size=0.2, SEED=42):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.population_size = population_size\n",
    "        self.num_generations = num_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.history = []\n",
    "        self.SEED = SEED\n",
    "        \n",
    "        # Setup data\n",
    "        self.X_tensor = torch.FloatTensor(X_data)\n",
    "        self.y_tensor = torch.LongTensor(y_data)\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X_tensor, self.y_tensor, test_size=test_size, random_state=SEED, stratify=self.y_tensor\n",
    "        )\n",
    "        \n",
    "        self.train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        self.test_dataset = TensorDataset(self.X_test, self.y_test)\n",
    "        \n",
    "        # Initialize population\n",
    "        self.population = [Gene() for _ in range(population_size)]\n",
    "        \n",
    "    def model_trainer(self, gene):\n",
    "        \"\"\"\n",
    "        Trains the models in the generation using the gener's hyperparameters.\n",
    "    \n",
    "        \"\"\"\n",
    "        # Use the gene's number of epochs\n",
    "        epochs = gene.num_epochs\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = gene.model_type(self.input_size, gene.hidden_size, gene.num_layers, self.num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=gene.lr)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                _, logits = model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def evaluate_model(self, model, dataset):\n",
    "        \"\"\"\n",
    "        Evaluates the model's classification accuracies.\n",
    "    \n",
    "        \"\"\"\n",
    "        data_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                _, logits = model(inputs)\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return correct / total\n",
    "    \n",
    "    def calculate_fitness(self, gene, model=None):\n",
    "        \"\"\"\n",
    "        Calculates the objective fitness of the gene based on train/test accuracies. \n",
    "    \n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            # Train model\n",
    "            model = self.model_trainer(gene)\n",
    "        \n",
    "        # Evaluate on train and test sets\n",
    "        train_accuracy = self.evaluate_model(model, self.train_dataset)\n",
    "        test_accuracy = self.evaluate_model(model, self.test_dataset)\n",
    "        \n",
    "        # Store accuracies in the gene\n",
    "        gene.train_accuracy = train_accuracy\n",
    "        gene.test_accuracy = test_accuracy\n",
    "        \n",
    "        # Calculate fitness with emphasis on test accuracy and generalization\n",
    "        # Penalize if train >> test (overfitting)\n",
    "        overfitting_penalty = max(0, train_accuracy - test_accuracy) * 0.25\n",
    "        \n",
    "        # Fitness is primarily test accuracy with a penalty for overfitting\n",
    "        fitness = test_accuracy - overfitting_penalty\n",
    "        gene.fitness = fitness\n",
    "        \n",
    "        return fitness\n",
    "    \n",
    "    def select_parents(self, num_parents):\n",
    "        \"\"\"\n",
    "        Selects best fit parents for crossover.\n",
    "    \n",
    "        \"\"\"\n",
    "        parents = []\n",
    "        \n",
    "        for _ in range(num_parents):\n",
    "            # Tournament selection\n",
    "            tournament_size = min(3, len(self.population))\n",
    "            tournament = random.sample(self.population, tournament_size)\n",
    "            tournament.sort(key=lambda x: x.fitness, reverse=True)\n",
    "            parents.append(deepcopy(tournament[0]))\n",
    "        \n",
    "        return parents\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        \"\"\"\n",
    "        Crossing over parents' hyperparameters at random.\n",
    "    \n",
    "        \"\"\"\n",
    "        if random.random() > self.crossover_rate:\n",
    "            return deepcopy(parent1)\n",
    "        \n",
    "        child = Gene()\n",
    "        \n",
    "        # Inherit traits with 50% chance from each parent\n",
    "        child.model_type = parent1.model_type if random.random() < 0.5 else parent2.model_type\n",
    "        child.num_epochs = parent1.num_epochs if random.random() < 0.5 else parent2.num_epochs\n",
    "        child.lr = parent1.lr if random.random() < 0.5 else parent2.lr\n",
    "        child.hidden_size = parent1.hidden_size if random.random() < 0.5 else parent2.hidden_size\n",
    "        child.num_layers = parent1.num_layers if random.random() < 0.5 else parent2.num_layers\n",
    "        \n",
    "        return child\n",
    "    \n",
    "    def mutate(self, gene):\n",
    "        \"\"\"\n",
    "        Randomly mutates genes based on mutation rate. \n",
    "    \n",
    "        \"\"\"\n",
    "        # Create a copy for mutation\n",
    "        mutated_gene = deepcopy(gene)\n",
    "        \n",
    "        # Randomly mutate each parameter based on mutation rate\n",
    "        if random.random() < self.mutation_rate:\n",
    "            mutated_gene.model_type = random.choice([RNN, LSTM])\n",
    "        \n",
    "        if random.random() < self.mutation_rate:\n",
    "            mutated_gene.num_epochs = max(5, mutated_gene.num_epochs + random.randint(-10, 10))\n",
    "        \n",
    "        if random.random() < self.mutation_rate:\n",
    "            learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "            current_index = learning_rates.index(mutated_gene.lr) if mutated_gene.lr in learning_rates else 0\n",
    "            new_index = max(0, min(len(learning_rates) - 1, current_index + random.randint(-1, 1)))\n",
    "            mutated_gene.lr = learning_rates[new_index]\n",
    "        \n",
    "        if random.random() < self.mutation_rate:\n",
    "            hidden_sizes = [16, 32, 64, 128, 256]\n",
    "            current_index = hidden_sizes.index(mutated_gene.hidden_size) if mutated_gene.hidden_size in hidden_sizes else 0\n",
    "            new_index = max(0, min(len(hidden_sizes) - 1, current_index + random.randint(-1, 1)))\n",
    "            mutated_gene.hidden_size = hidden_sizes[new_index]\n",
    "        \n",
    "        if random.random() < self.mutation_rate:\n",
    "            mutated_gene.num_layers = max(1, mutated_gene.num_layers + random.randint(-1, 1))\n",
    "        \n",
    "        return mutated_gene\n",
    "    \n",
    "    def evolve_population(self):\n",
    "        \"\"\"\n",
    "        Establishing new generation of models. \n",
    "    \n",
    "        \"\"\"\n",
    "        # Calculate fitness for each gene\n",
    "        for gene in self.population:\n",
    "            if gene.fitness == 0:  \n",
    "                self.calculate_fitness(gene)\n",
    "        \n",
    "        # Sort population by fitness\n",
    "        self.population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "        \n",
    "        # Store best gene from this generation\n",
    "        best_gene = deepcopy(self.population[0])\n",
    "        self.history.append(best_gene)\n",
    "        \n",
    "        # Select parents\n",
    "        num_parents = max(2, self.population_size // 2)\n",
    "        parents = self.select_parents(num_parents)\n",
    "        \n",
    "        # Create new population\n",
    "        new_population = []\n",
    "        \n",
    "        # Keep the best gene\n",
    "        new_population.append(deepcopy(best_gene))\n",
    "        \n",
    "        # Create children through crossover and mutation\n",
    "        while len(new_population) < self.population_size:\n",
    "            parent1 = random.choice(parents)\n",
    "            parent2 = random.choice(parents)\n",
    "            \n",
    "            child = self.crossover(parent1, parent2)\n",
    "            child = self.mutate(child)\n",
    "            \n",
    "            new_population.append(child)\n",
    "        \n",
    "        self.population = new_population\n",
    "    \n",
    "    def run(self):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"Starting genetic algorithm with {self.population_size} individuals for {self.num_generations} generations\")\n",
    "        print(f\"This can take upwards of 10 minutes\")\n",
    "        \n",
    "        for generation in range(self.num_generations):\n",
    "            gen_start_time = time.time()\n",
    "            print(f\"\\nGeneration {generation+1}/{self.num_generations}\")\n",
    "            \n",
    "            self.evolve_population()\n",
    "            \n",
    "            # Print best performing gene in this generation\n",
    "            best_gene = self.history[-1]\n",
    "            print(f\"Best in generation {generation+1}: {best_gene}\")\n",
    "            \n",
    "            gen_end_time = time.time()\n",
    "            print(f\"Generation time: {gen_end_time - gen_start_time:.2f} seconds\")\n",
    "        \n",
    "        # Get best gene across all generations\n",
    "        best_gene = max(self.history, key=lambda x: x.fitness)\n",
    "        \n",
    "        # Train the final model with the best gene\n",
    "        print(\"\\nTraining final model with best hyperparameters...\")\n",
    "        best_model = self.model_trainer(best_gene)\n",
    "        \n",
    "        # Evaluate final model\n",
    "        train_accuracy = self.evaluate_model(best_model, self.train_dataset)\n",
    "        test_accuracy = self.evaluate_model(best_model, self.test_dataset)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Genetic Algorithm completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Best hyperparameters found:\")\n",
    "        print(best_gene)\n",
    "        print(f\"Final Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        return best_gene, best_model, self.history\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"\n",
    "        Plotting fitness and accuracies of top gene model from each generation.\n",
    "    \n",
    "        \"\"\"\n",
    "        generations = list(range(1, len(self.history) + 1))\n",
    "        fitness_values = [gene.fitness for gene in self.history]\n",
    "        train_acc = [gene.train_accuracy for gene in self.history]\n",
    "        test_acc = [gene.test_accuracy for gene in self.history]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(generations, fitness_values, 'b-', marker='o')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('Fitness')\n",
    "        plt.title('Evolution of Fitness')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(generations, train_acc, 'g-', marker='o', label='Train Accuracy')\n",
    "        plt.plot(generations, test_acc, 'r-', marker='o', label='Test Accuracy')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Evolution of Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_results_table(self):\n",
    "        \"\"\"\n",
    "        Outputs results. \n",
    "    \n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i, gene in enumerate(self.history):\n",
    "            gene_dict = gene.to_dict()\n",
    "            gene_dict[\"generation\"] = i+1\n",
    "            results.append(gene_dict)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Function to run the genetic algorithm\n",
    "def run_genetic_algorithm(X_data, y_data, input_size, num_classes, population_size, num_generations,seed=42):\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    # Initialize and run the genetic algorithm\n",
    "    ga = GeneticAlgorithm(\n",
    "        X_data=X_data,\n",
    "        y_data=y_data,\n",
    "        input_size=input_size,\n",
    "        num_classes=num_classes,\n",
    "        population_size=population_size,\n",
    "        num_generations=num_generations,\n",
    "        mutation_rate=0.2,\n",
    "        crossover_rate=0.7,\n",
    "        batch_size=16,\n",
    "        test_size=0.2,\n",
    "        SEED=SEED\n",
    "    )\n",
    "    \n",
    "    best_gene, best_model, history = ga.run()\n",
    "    \n",
    "    ga.plot_history()\n",
    "    \n",
    "    results = ga.get_results_table()\n",
    "    \n",
    "    return best_gene, best_model, results\n",
    "\n",
    "X_data, y_data, pitch_type_to_idx, idx_to_pitch_type = prepare_data(dataframes)\n",
    "\n",
    "input_size = X_data.shape[2]  # Number of features\n",
    "num_classes = len(set(y_data))\n",
    "\n",
    "best_gene, best_model, results = run_genetic_algorithm(X_data, y_data, input_size, num_classes, population_size=30, num_generations=5,seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set flag to 'True' if you want to save your best model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = False\n",
    "\n",
    "save_model(best_model, 'rnn_lstm_model', save_flag, directory=save_your_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Model\n",
    "\n",
    "This section implements a powerful architecture typically used for image classification:\n",
    "\n",
    "- **Convolutional Neural Network (CNN)**: Effective for capturing image patterns [Read more...](https://www.ibm.com/think/topics/convolutional-neural-networks)\n",
    "\n",
    "Observe the training and test set loss/epoch and accuracy/epoch below. Choose an epoch range to stop training the model where the train and test set losses/accuracies both improve before divergence. While the training set loss continues to go down resembling an improvement in predictive accuracy, that just represents the model overfitting to the training data. \n",
    "\n",
    "Convolutional Neural Networks are typically used for image classification. In essence, the landmark coordinate graphs which you can observe in the Data_Exploration file \"paint a picture\" of the pitcher's motions which the CNN can train with. This CNN model does not explicitly use temporal patterns the way RNN's and LSTM's do but implicitly does by looking at overall landmark trajectories' patterns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, seq_len):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Reshape input to have 1 channel with height=seq_len and width=input_size\n",
    "        self.input_channels = 1\n",
    "        self.input_height = seq_len\n",
    "        self.input_width = input_size\n",
    "        \n",
    "        # Conv2D layers with square filters (3x3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate flattened size after 3 pooling layers\n",
    "        self.flattened_size = self._get_flattened_size(seq_len, input_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flattened_size, 256)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def _get_flattened_size(self, seq_len, input_size):\n",
    "        # Calculate dimensions after 3 pooling layers (each reduces size by factor of 2)\n",
    "        height_after_pools = seq_len // 2 // 2 // 2\n",
    "        width_after_pools = input_size // 2 // 2 // 2\n",
    "        return 128 * height_after_pools * width_after_pools\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input: [batch_size, seq_len, input_size] -> [batch_size, 1, seq_len, input_size]\n",
    "        x = x.unsqueeze(1)  \n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool3(x)\n",
    "    \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        \n",
    "        return prob, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with loss and accuracy tracking\n",
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Setting up a CNN classifier pipeline. \n",
    "    \n",
    "    Parameters:\n",
    "    model: CNN model\n",
    "    train_loader: Training data\n",
    "    val_loader: Validation data\n",
    "    num_epochs (int): Number of epochs\n",
    "    learning_rate (float): Learning rate \n",
    "    \n",
    "    Outputs:\n",
    "    model: Trained CNN model\n",
    "    history (dict): Dictionary of training statistics such as loss and accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Cross Entropy Loss and Adam Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Lists to store metrics for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    epochs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            probs, logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                probs, logits = model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_acc = 100 * correct / total\n",
    "        \n",
    "        # Store metrics for plotting\n",
    "        epochs.append(epoch + 1)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Test Loss: {epoch_val_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, Test Acc: {epoch_val_acc:.2f}%')\n",
    "    \n",
    "    # Create dictionary with training history\n",
    "    history = {\n",
    "        'epochs': epochs,\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'train_acc': train_accuracies,\n",
    "        'val_acc': val_accuracies\n",
    "    }\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['epochs'], history['train_loss'], 'b-o', label='Training Loss')\n",
    "    plt.plot(history['epochs'], history['val_loss'], 'r-o', label='Test Loss')\n",
    "    plt.title('CNN Loss vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (Cross Entropy)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['epochs'], history['train_acc'], 'g-o', label='Training Accuracy')\n",
    "    plt.plot(history['epochs'], history['val_acc'], 'm-o', label='Test Accuracy')\n",
    "    plt.title('CNN Accuracy vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup data\n",
    "X_tensor = torch.FloatTensor(X_data)\n",
    "y_tensor = torch.LongTensor(y_data)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tensor, y_tensor, test_size=0.2, random_state=SEED, stratify=y_tensor\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change number of epochs here\n",
    "'___________________'\n",
    "num_epochs = 50\n",
    "'___________________'\n",
    "\n",
    "model = CNN(input_size=X_data.shape[2], num_classes=len(torch.unique(y_train)), seq_len=24)\n",
    "cnn_model, history = train_model(model, train_loader, test_loader, num_epochs)\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set flag to 'True' if you want to save your best model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = False\n",
    "\n",
    "save_model(cnn_model, 'cnn_model', save_flag, directory=save_your_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS100Program1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
